# RNN
We would like to experiment with the Penn Tree Bank data set. In this data, we predict the next word. Instead of accuracy, we measure the performance by perplexity, which is quite similar to cross-entropy loss done in classification based NN. For architecture, we use RNN Regularization model as described in Zaremba et al. 
